[{"categories":["Engineering"],"contents":"I recently interviewed for a senior engineering role at a well-known fintech company. The interview went sideways in an interesting way, and the feedback I received reveals something important about how technical interviews can fail when leveling expectations aren\u0026rsquo;t aligned.\nThe Interview The system design question was straightforward: \u0026ldquo;Design a system that displays the top 5 songs currently being played in real-time across our streaming platform.\u0026rdquo;\nI proposed a solution using:\nHash ring sharding (by user_id to avoid hot partitions on popular songs) State-based tracking with 1-hour TTL (not time-windowed aggregation) Gossip protocol for service membership and discovery Optimistic locking for Redis cache updates to handle concurrent writes HTTP Cache-Control headers (max-age, stale-while-revalidate) for client-side caching Redis caching layer to prevent thundering herd problems and act as a circuit breaker The design aimed for eventual consistency without requiring consensus protocols, with self-healing through TTL expiry and no single point of failure.\nHere\u0026rsquo;s the architecture flow:\nStreaming Users (10M concurrent) ↓ Global Kafka Stream (playing/stopped events) ↓ Hash Ring (shard by user_id) ↓ ┌─────────┬─────────┬─────────┐ │ Shard 1 │ Shard 2 │ Shard 3 │ ← Each shard tracks songs │ Users │ Users │ Users │ for its user bucket │ 1-33% │ 34-66% │ 67-100% │ └─────────┴─────────┴─────────┘ ↑ Gossip Protocol (Serf/Consul-style membership) ↓ Aggregation Service - Queries all shards: \u0026#34;Give me your top 5\u0026#34; - Merges results across shards - Calculates global top 5 ↓ Redis Cache (optimistic locking via WATCH/MULTI/EXEC) ↓ GET /top-songs API Cache-Control: max-age=10, stale-while-revalidate=5 ↓ Clients (HTTP caching) Key design decisions:\nState-based tracking (not time windows): Track who is currently playing what, with 1-hour TTL Shard by user_id (not song_id): Avoids hot partitions from viral songs Gossip membership: Automatic failure detection and self-healing Optimistic locking: Prevents race conditions on cache updates Multi-layer caching: Redis + HTTP + client-side During the interview, I noticed the interviewer seemed surprised by several concepts. They asked clarifying questions about hash rings, seemed unfamiliar with optimistic locking patterns, and appeared genuinely interested when I mentioned HTTP caching strategies - as if this was new information rather than standard practice.\nThe Feedback A few days later, I received the rejection:\n\u0026ldquo;They reached a basic workable system design, although it was difficult to maintain shared understanding with the candidate. They repeatedly used buzzwords/terminology without stopping to check in with the interviewer to ensure they were following/understood.\u0026rdquo;\nHere\u0026rsquo;s what stopped me: Hash rings, gossip protocols, optimistic locking, and thundering herd aren\u0026rsquo;t buzzwords. They\u0026rsquo;re fundamental distributed systems concepts that any senior engineer working on distributed systems should recognize, even if they don\u0026rsquo;t use them daily.\nThese terms exist because they describe specific, well-understood technical problems and solutions:\nHash ring = consistent hashing for distributed key-value storage Gossip protocol = decentralized membership and failure detection Optimistic locking = conflict resolution without blocking Thundering herd = cache stampede problem when many clients simultaneously request cold cache data Using the correct terminology isn\u0026rsquo;t \u0026ldquo;buzzwords\u0026rdquo; - it\u0026rsquo;s being precise.\nThe Real Problem: Leveling Mismatch The feedback framed the interviewer\u0026rsquo;s knowledge gaps as my communication failure. But here\u0026rsquo;s the thing - I wasn\u0026rsquo;t throwing around trendy terms to sound impressive. I was describing actual implementation patterns that would be necessary for the system they asked me to design.\nLater, I learned the interviewer had 7 years at the company working on payment systems (BACS, Faster Payments, Mastercard integration). These are complex systems, but they hadn\u0026rsquo;t worked with modern distributed systems architecture patterns. Kubernetes made them uncomfortable, and concepts like gossip protocols, hash rings, and optimistic locking weren\u0026rsquo;t part of their domain experience.\nThis wasn\u0026rsquo;t a communication problem. It was a leveling problem.\nThe company was hiring for a staff-level distributed systems role, but sent an engineer whose domain experience was in payment systems integration to evaluate distributed systems architecture. The interviewer couldn\u0026rsquo;t distinguish between legitimate technical depth and \u0026ldquo;buzzwords\u0026rdquo; because they lacked the context to evaluate either.\nThe \u0026ldquo;Ego\u0026rdquo; Red Flag The feedback also mentioned:\n\u0026ldquo;Whilst the behavioural persuasion example was okay, they mentioned the main obstacle to others being persuaded of his point of view was their egos, which was a bit of a red flag.\u0026rdquo;\nThe question was: \u0026ldquo;Tell me about a time you\u0026rsquo;ve convinced others about something technical. How did you approach it?\u0026rdquo;\nI talked about introducing GitOps patterns (mono-repo, submodules) to teams that had been working in silos. My answer was:\n\u0026ldquo;Most people are hesitant at first - it seems like a lot when you\u0026rsquo;ve been siloed for a long time. But I explain how it breaks down walls between departments because everyone can see and work on all the code. Even if you don\u0026rsquo;t need to touch other team\u0026rsquo;s code, it helps with communication and understanding. 99% of people love it once they see the benefits. But there\u0026rsquo;s always that 1% - the ones you couldn\u0026rsquo;t convince to write unit tests or follow SOLID principles either. Usually it\u0026rsquo;s ego - the \u0026lsquo;rockstar developer\u0026rsquo; types who think best practices don\u0026rsquo;t apply to them.\u0026rdquo;\nI was describing the 1% of engineers who resist all best practices, not saying \u0026ldquo;everyone who disagreed with me had an ego problem.\u0026rdquo;\nBut in an interview where the interviewer was already feeling technically out of their depth, that answer probably landed as: \u0026ldquo;This person thinks they\u0026rsquo;re smarter than everyone else and dismisses people who don\u0026rsquo;t agree.\u0026rdquo;\nContext matters. Even a reasonable answer can sound arrogant when the interviewer is already defensive.\nWhat I Learned 1. Interview leveling matters both ways\nCompanies need to ensure interviewers can actually evaluate the level they\u0026rsquo;re hiring for. A mid-level engineer shouldn\u0026rsquo;t be the primary technical evaluator for a staff-level distributed systems role - not because they\u0026rsquo;re not smart, but because they literally can\u0026rsquo;t distinguish signal from noise at that level.\n2. \u0026ldquo;Buzzwords\u0026rdquo; is sometimes code for \u0026ldquo;concepts I don\u0026rsquo;t know\u0026rdquo;\nWhen feedback says you \u0026ldquo;used buzzwords,\u0026rdquo; ask yourself: Were you actually using jargon unnecessarily, or were you using precise technical terminology that the interviewer wasn\u0026rsquo;t familiar with? There\u0026rsquo;s a meaningful difference.\n3. Calibrate to your audience - even in interviews\nI could have probed more during the interview: \u0026ldquo;Are you familiar with hash ring sharding? Let me explain how it works\u0026hellip;\u0026rdquo; But that\u0026rsquo;s a delicate dance - you don\u0026rsquo;t want to sound condescending, and in a high-stakes interview, you\u0026rsquo;re trying to demonstrate competence, not teach concepts.\n4. Word choice matters in behavioral questions\nSaying \u0026ldquo;their egos got in the way\u0026rdquo; is different from saying \u0026ldquo;there was initial resistance to changing direction, but once we did a proof-of-concept, everyone aligned.\u0026rdquo; Same situation, drastically different framing.\nThe Unexpected Twist Oh, and there was one more thing: The recruiter also flagged that the interviewer was \u0026ldquo;95% confident I was drinking beer\u0026rdquo; during the 9am Tuesday video call.\nI was drinking Robinsons summer fruits squash (pink liquid) in a Beavertown-branded pint glass that was a Christmas present.\nI sent a polite clarification email immediately. The recruiter responded the following afternoon confirming HR had checked with the interviewer - it didn\u0026rsquo;t affect the decision. I\u0026rsquo;d already failed on technical \u0026ldquo;communication\u0026rdquo; grounds (the \u0026ldquo;buzzwords\u0026rdquo; feedback).\nBut it\u0026rsquo;s a useful reminder that video interview optics matter, even when you\u0026rsquo;re just trying to stay hydrated with pink squash at 9am on a Tuesday.\nThe Good Rejection Here\u0026rsquo;s the thing: This was a good rejection for both of us.\nIf their senior engineers don\u0026rsquo;t regularly use distributed systems concepts like hash rings and gossip protocols, then my experience wouldn\u0026rsquo;t have been valued there anyway. I would have been frustrated, they would have been frustrated, and we\u0026rsquo;d both be worse off.\nThe feedback about \u0026ldquo;buzzwords\u0026rdquo; told me everything I needed to know about the technical depth of the role and the team I\u0026rsquo;d be joining.\nFor Other Candidates If you get feedback that you \u0026ldquo;used too many buzzwords\u0026rdquo; or \u0026ldquo;didn\u0026rsquo;t explain concepts clearly,\u0026rdquo; consider:\nWere you actually using jargon unnecessarily? Sometimes we do get carried away with terminology when simpler language would suffice.\nOr were you using precise technical language that the interviewer wasn\u0026rsquo;t familiar with? In which case, the leveling might not be right for that role.\nDid you probe for understanding during the interview? I could have checked in more often with \u0026ldquo;Does that make sense?\u0026rdquo; or \u0026ldquo;Are you familiar with this pattern?\u0026rdquo;\nIs this a role where your depth would be valued? If they\u0026rsquo;re calling fundamental distributed systems concepts \u0026ldquo;buzzwords,\u0026rdquo; they might not be working on the kinds of problems you want to solve.\nFor Hiring Companies If you\u0026rsquo;re hiring for senior/staff level distributed systems roles:\nEnsure your interviewers can evaluate at that level. Don\u0026rsquo;t send someone who primarily works on monoliths to evaluate distributed systems expertise.\nTrain interviewers to distinguish between buzzwords and technical depth. \u0026ldquo;Eventual consistency\u0026rdquo; isn\u0026rsquo;t a buzzword when you\u0026rsquo;re literally designing an eventually-consistent system.\nCalibrate your leveling expectations. If your team doesn\u0026rsquo;t regularly use concepts like hash rings, CRDTs, or consensus protocols, you might not actually need staff-level distributed systems expertise - and that\u0026rsquo;s fine! Just don\u0026rsquo;t interview as if you do.\nHave you experienced interview leveling mismatches? I\u0026rsquo;d love to hear your stories - both as a candidate and as an interviewer. What are the signs that an interviewer can\u0026rsquo;t evaluate the level they\u0026rsquo;re hiring for?\nUpdate: I\u0026rsquo;m currently exploring roles where distributed systems depth is actually valued. If you\u0026rsquo;re working on interesting problems in this space, let\u0026rsquo;s connect.\n","date":"January 15, 2026","hero":"/images/default-hero.jpg","permalink":"/posts/when-buzzwords-arent-buzzwords/","summary":"\u003cp\u003eI recently interviewed for a senior engineering role at a well-known fintech company. The interview went sideways in an interesting way, and the feedback I received reveals something important about how technical interviews can fail when leveling expectations aren\u0026rsquo;t aligned.\u003c/p\u003e\n\u003ch2 id=\"the-interview\"\u003eThe Interview\u003c/h2\u003e\n\u003cp\u003eThe system design question was straightforward: \u003cstrong\u003e\u0026ldquo;Design a system that displays the top 5 songs currently being played in real-time across our streaming platform.\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eI proposed a solution using:\u003c/p\u003e","tags":["interviews","distributed-systems","career","system-design"],"title":"When 'Buzzwords' Reveal an Interview Leveling Mismatch"},{"categories":null,"contents":"Introduction\nIf you’ve spent any time working with Kubernetes, you’ve probably heard of GitOps -a methodology that treats Git as the source of truth for defining and operating infrastructure and applications. In this post, I’ll walk you through a GitOps setup that uses a hierarchical folder structure, combining Helm, Helmfile, and Kustomize to give you robust, testable, and scalable deployments. We’ll also see how tools like Flux and Tilt fit into the workflow, enabling both automated deployments and seamless local development.\nWhy GitOps? Before we dive into the specifics, let’s revisit what GitOps brings to the table:\nVersion Control: Every change to your Kubernetes configurations is committed to Git, providing an audit trail and easy rollbacks. Single Source of Truth: Teams can rely on the repo as the canonical description of what’s running in each cluster. Automation: Changes in Git trigger updates to your clusters, reducing manual operations and ensuring consistency. This post assumes you\u0026rsquo;re already sold on GitOps and are looking for a tangible organisational pattern. Let\u0026rsquo;s jump in.\nThe Repository Structure Our GitOps repository is divided into four main folders -plus a special src directory for source code and a Tiltfile for local dev. Here’s a quick overview:\n. ├── helm-charts ├── components ├── groups ├── clusters ├── src └── Tiltfile 1. Helm Charts Purpose: This directory stores all Helm charts -whether first-party or third-party dependencies -that form the foundation of your Kubernetes services. Workflow: Render: Helm charts are templated to disk. Include in Kustomize: You use Kustomize to ingest those templates. Manage with Helmfile: Helmfile can orchestrate multiple Helm releases, ensuring everything is installed/updated consistently. This approach decouples the raw Helm charts from the environment-specific overlays, making it easier to plug them into different clusters in a standardized way.\n2. Components Purpose: Store Kubernetes manifests that are not part of Helm charts. This could include CRDs (Custom Resource Definitions), operator manifests, or any other resources you want to keep separate. Usage: Directly reference these components in your cluster definitions or in a higher-level grouping concept (more on that next). 3. Groups Purpose: Group related services and configurations together under a single overlay. For example, a monitoring group might include Prometheus, Grafana, and other supporting components. Hierarchy: Groups can reference other groups, enabling layering. A dev group might inherit from a default group, adding environment-specific patches for development clusters (e.g., less resource allocation, debug logging). 4. Clusters Purpose: Each cluster folder describes exactly what should be running on that cluster, pulling in components and groups as needed. Structure: Each cluster has its own folder, which Flux (or another GitOps tool like Argo CD) monitors. Subfolders often map to namespaces or functional areas. Environment-specific customisations, such as image overrides or domain-specific settings, also live here. Benefits: This design ensures that each cluster references only the resources it needs, with any environment-specific overrides captured in a single place. 5. src (Git Submodules) Purpose: Each application or service your team develops has its own dedicated repo, added to this GitOps repo as a submodule. Motivation: Separating source code lifecycles from infrastructure while still keeping them in close proximity. Each service can evolve at its own pace (with separate versioning and pull requests). When you’re ready to deploy, you update references in the GitOps repo to point to the correct version or commit. 6. Tilt for Local Development Tiltfile: A single Tiltfile at the root of your GitOps repo configures local Kubernetes development using Tilt and k3d. Realtime Feedback: Tilt builds Docker images locally as you code and pushes them into your k3d cluster. You can check out feature branches across multiple submodules and test them all together in a local environment. Developer Happiness: This local dev approach drastically shortens the feedback loop, letting you iterate faster than if you had to push and wait for a remote pipeline to run. CI/CD Flow Now that we’ve broken down the structure, let’s see how changes flow from a developer’s pull request to a cluster.\n1. Pull Request → Image Build Trigger: A developer creates or updates a pull request in the src project repository. Automation: A CI pipeline (e.g., GitHub Actions, Jenkins, GitLab CI) builds a Docker image for the new code. 2. CI Environment Setup in Dev Cluster The pipeline references the GitOps repo (specifically the Dev cluster folder). A CI folder under that Dev cluster is used to stand up a temporary environment for tests. This CI folder typically isn’t referenced by the main Dev cluster overlay, so it doesn’t affect production-like resources. The pipeline applies a Kustomization overlay that includes the new Docker image (and possibly “latest” versions of other services). 3. Readiness \u0026amp; Integration Tests Wait for Ready: The pipeline checks that all pods in the CI environment reach a “Ready” state. Integration Tests: Another folder within the CI path (e.g., integration-test) includes job manifests that run your test suite. The pipeline applies these manifests, waits for the jobs to complete, then collects logs/results. 4. Cleanup Once tests finish, the pipeline tears down the temporary namespace to keep clusters clean. If tests pass, the pipeline can merge the pull request or notify that the new image is ready for promotion. Key Benefits Modular \u0026amp; Extensible: By separating Helm charts, components, groups, and clusters, you can easily add new resources or reuse existing ones. Consistent Environments: Groups let you define and share sets of configurations across multiple stages (e.g., dev, staging, prod). Automated Testing: The CI process ensures each new feature or fix is validated in an ephemeral environment, mirroring production as closely as you need. Local Development: Tilt and k3d let you replicate the cluster environment on your machine, enabling quicker feedback loops and more productive debugging. Auditability \u0026amp; Traceability: Since every change is committed to Git, you get a clear history of who changed what and when. The Single Pane of Glass Beyond the technical benefits, this pattern fundamentally changes how teams collaborate.\nOne entry point, any role. Whether you\u0026rsquo;re a backend engineer, frontend dev, SRE, or even a PM wanting to understand the system - you clone one repo and run tilt up. Within minutes you have a working https://localhost.company.co/ with the entire stack running locally.\nCross-functional debugging. I\u0026rsquo;ve watched frontend engineers fix backend bugs by running grep -r \u0026quot;error message\u0026quot; from the repo root. The pseudo mono-repo structure means the answer is always somewhere in your checkout - you just need to find it.\nNo onboarding tax. Switching teams doesn\u0026rsquo;t mean spending a week setting up a new development environment. The same tilt up command works whether you\u0026rsquo;re on payments, auth, or the data pipeline.\nDeclarative infrastructure as documentation. New joiners can understand the entire system topology by reading the clusters/ and groups/ directories. The infrastructure is the documentation.\nConclusion Adopting GitOps with a well-thought-out repository structure can dramatically simplify your Kubernetes workflows. By combining Helm, Helmfile, Kustomize, and tools like Flux or Argo CD, you can create modular, scalable, and testable deployments. And with a local development pipeline powered by Tilt and k3d, you can iterate quickly without sacrificing best practices.\nIf you’re looking for a GitOps pattern that balances clarity, flexibility, and collaboration, give this structure a try. You’ll enjoy:\nFewer manual steps. A predictable CI process. An environment that’s friendly for both new and experienced team members. Ready to dive deeper? Experiment with a small cluster or side project first. Once you’re comfortable with the structure, scale it up to your full production workloads. Happy deploying!\nFurther Reading\nFlux CD Helm Kustomize Tilt k3d Docs ","date":"February 25, 2025","hero":"/posts/gitops/hero.jpg","permalink":"/posts/gitops/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003cbr\u003e\nIf you’ve spent any time working with Kubernetes, you’ve probably heard of \u003cem\u003eGitOps\u003c/em\u003e -a methodology that treats Git as the source of truth for defining and operating infrastructure and applications. In this post, I’ll walk you through a GitOps setup that uses a hierarchical folder structure, combining Helm, Helmfile, and Kustomize to give you robust, testable, and scalable deployments. We’ll also see how tools like Flux and Tilt fit into the workflow, enabling both automated deployments and seamless local development.\u003c/p\u003e","tags":null,"title":"Practical GitOps Pattern"},{"categories":null,"contents":"Introduction A common pattern I see promoted is using tools that show you what will change in your cluster at sync time - after your code is already merged. In my view, this is already too late and goes against GitOps principles. How can Git be the source of truth if there are extra steps between merge and understanding impact?\nIn this post, I\u0026rsquo;ll show you how to generate manifest diffs during PR review, so reviewers see exactly what will change in the cluster before they approve.\nThe Problem Consider the typical Argo CD diff plugin workflow:\nDeveloper creates PR Reviewer approves (without seeing cluster impact) Code merges Now you see the diff of what will change Sync happens The diff comes too late. The reviewer has already approved. If something looks wrong, you need another PR to fix it.\nGitOps should mean: what\u0026rsquo;s in Git is what\u0026rsquo;s in the cluster. Reviewers should understand the full impact before approving.\nThe Solution Generate diffs at PR time by:\nBuilding manifests from the main branch Building manifests from the PR branch Diffing the two Posting the result as an artifact or comment This way, reviewers see the actual Kubernetes resources that will change - deployments, services, configmaps, everything.\nKey Principle: Absorb Your Helm Charts Before we can diff manifests, we need actual manifests to diff. If you\u0026rsquo;re deploying Helm charts directly via helm install, you don\u0026rsquo;t have rendered manifests in Git.\nInstead, absorb them:\nhelm template my-release my-chart \\ --namespace my-namespace \\ \u0026gt; components/my-service/manifests.yaml Or better, use Helmfile to declaratively manage this:\n# helmfile.yaml - keep charts vanilla, use inline values only releases: - name: prometheus namespace: monitoring chart: prometheus-community/prometheus version: 25.0.0 # Minimal inline values - external values files don\u0026#39;t work with template Then render to disk:\nhelmfile template \u0026gt; components/monitoring/prometheus.yaml Important: Keep absorbed charts as vanilla as possible. Don\u0026rsquo;t try to configure everything via Helm values. Instead, push customisation to Kustomize overlays where changes are visible and diffable:\n# groups/monitoring/kustomization.yaml resources: - ../../components/monitoring/prometheus.yaml patches: - patch: |- - op: replace path: /spec/replicas value: 3 target: kind: Deployment name: prometheus-server This separation means:\nComponents: Vanilla absorbed charts (rarely change) Groups/Clusters: Environment-specific patches (visible in diffs) Building and Diffing The core CI logic is straightforward:\n# Build manifests from main branch git checkout origin/main kustomize build clusters/prod \u0026gt; /tmp/main-manifests.yaml # Build manifests from PR branch git checkout $PR_BRANCH kustomize build clusters/prod \u0026gt; /tmp/pr-manifests.yaml # Generate diff diff -u /tmp/main-manifests.yaml /tmp/pr-manifests.yaml \u0026gt; diff.txt For multiple clusters or kustomization roots, iterate:\nfor root in clusters/*/; do cluster=$(basename $root) kustomize build $root \u0026gt; /tmp/main-$cluster.yaml # ... diff each done GitLab CI Example generate-diffs: rules: - if: $CI_MERGE_REQUEST_IID changes: - deployments/**/* script: | # Fetch main branch git fetch origin main # Build main branch manifests git checkout origin/main for root in clusters/*/; do cluster=$(basename $root) kustomize build $root \u0026gt; /tmp/main-$cluster.yaml done # Build PR branch manifests git checkout $CI_COMMIT_SHA for root in clusters/*/; do cluster=$(basename $root) kustomize build $root \u0026gt; /tmp/pr-$cluster.yaml diff -u /tmp/main-$cluster.yaml /tmp/pr-$cluster.yaml \u0026gt; diffs/$cluster.diff || true done artifacts: paths: - diffs/ when: always GitHub Actions Example name: Generate Manifest Diffs on: pull_request: paths: - \u0026#39;clusters/**\u0026#39; - \u0026#39;components/**\u0026#39; - \u0026#39;groups/**\u0026#39; jobs: diff: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 with: fetch-depth: 0 - name: Setup Kustomize uses: imranismail/setup-kustomize@v2 - name: Generate diffs run: | mkdir -p diffs for root in clusters/*/; do cluster=$(basename $root) # Build from main git checkout origin/main kustomize build $root \u0026gt; /tmp/main-$cluster.yaml 2\u0026gt;/dev/null || echo \u0026#34;\u0026#34; \u0026gt; /tmp/main-$cluster.yaml # Build from PR git checkout ${{ github.sha }} kustomize build $root \u0026gt; /tmp/pr-$cluster.yaml # Diff diff -u /tmp/main-$cluster.yaml /tmp/pr-$cluster.yaml \u0026gt; diffs/$cluster.diff || true done - name: Upload diffs uses: actions/upload-artifact@v4 with: name: manifest-diffs path: diffs/ - name: Comment on PR uses: actions/github-script@v7 with: script: | const fs = require(\u0026#39;fs\u0026#39;); const diffs = fs.readdirSync(\u0026#39;diffs\u0026#39;); let comment = \u0026#39;## Manifest Diffs\\n\\n\u0026#39;; for (const file of diffs) { const content = fs.readFileSync(`diffs/${file}`, \u0026#39;utf8\u0026#39;); if (content.trim()) { comment += `\u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;${file}\u0026lt;/summary\u0026gt;\\n\\n\\`\\`\\`diff\\n${content}\\n\\`\\`\\`\\n\u0026lt;/details\u0026gt;\\n\\n`; } } github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: comment }); Finding Kustomization Roots In simple setups (one kustomization per cluster), iterating clusters/*/ works fine. But in more complex setups with nested kustomizations, groups referencing groups, and shared components, you need to find the roots - kustomizations that aren\u0026rsquo;t referenced by any other kustomization.\nThe algorithm:\nWalk all kustomization.yaml files, mark each as a potential root For each one, look at its resources and bases Any kustomization that\u0026rsquo;s referenced by another gets marked as NOT a root What remains are true roots Here\u0026rsquo;s a bash implementation:\n#!/usr/bin/env bash # find-kustomize-roots.sh - Find kustomizations not referenced by others declare -A is_root # Find all kustomization.yaml files and mark as potential roots while IFS= read -r kfile; do is_root[\u0026#34;$kfile\u0026#34;]=1 done \u0026lt; \u0026lt;(find . -name \u0026#34;kustomization.yaml\u0026#34;) # For each kustomization, mark its references as non-roots for kfile in \u0026#34;${!is_root[@]}\u0026#34;; do dir=$(dirname \u0026#34;$kfile\u0026#34;) # Extract resources and bases from the kustomization refs=$(yq -r \u0026#39;(.resources // []) + (.bases // []) | .[]\u0026#39; \u0026#34;$kfile\u0026#34; 2\u0026gt;/dev/null) for ref in $refs; do # Resolve the referenced kustomization path ref_kustomize=$(realpath -m \u0026#34;$dir/$ref/kustomization.yaml\u0026#34; 2\u0026gt;/dev/null) ref_kustomize=\u0026#34;.${ref_kustomize#$(pwd)}\u0026#34; if [[ -f \u0026#34;$ref_kustomize\u0026#34; ]]; then is_root[\u0026#34;$ref_kustomize\u0026#34;]=0 fi done done # Output only the roots for kfile in \u0026#34;${!is_root[@]}\u0026#34;; do if [[ \u0026#34;${is_root[$kfile]}\u0026#34; == \u0026#34;1\u0026#34; ]]; then echo \u0026#34;$kfile\u0026#34; fi done Now your diff script becomes:\n# Build and diff only root kustomizations for root in $(./find-kustomize-roots.sh); do root_dir=$(dirname \u0026#34;$root\u0026#34;) name=$(echo \u0026#34;$root_dir\u0026#34; | tr \u0026#39;/\u0026#39; \u0026#39;-\u0026#39;) git checkout origin/main kustomize build \u0026#34;$root_dir\u0026#34; \u0026gt; /tmp/main-$name.yaml git checkout $PR_BRANCH kustomize build \u0026#34;$root_dir\u0026#34; \u0026gt; /tmp/pr-$name.yaml diff -u /tmp/main-$name.yaml /tmp/pr-$name.yaml \u0026gt; diffs/$name.diff || true done This ensures you diff what actually gets deployed, not intermediate layers that are only used as building blocks.\nMaking It Visual Plain diffs work, but an HTML visualization makes review easier:\nTree view of changed files/resources Side-by-side comparison Kubernetes metadata (kind, namespace, name) Collapsible sections for large changes You can build this with Go templates, React, or even a simple script that wraps diff2html.\nKey Benefits No Surprises: Reviewers see exactly what will change before approving Git Is Truth: The merged state is the deployed state - no extra steps Faster Reviews: Clear diffs mean faster, more confident approvals Catch Mistakes Early: Spot accidental changes (wrong image tag, missing resource limits) before they hit the cluster Audit Trail: Diffs become part of the PR history Conclusion Showing diffs at deployment time is too late. By the time you see the impact, the code is already merged. True GitOps means understanding the full impact during review.\nThe pattern is simple:\nAbsorb Helm charts into your repo as vanilla rendered manifests Kustomize via Kustomize overlays (not Helm values) Build kustomize roots for both branches Diff and present to reviewers Review with full knowledge of cluster impact This approach has saved me countless \u0026ldquo;oops\u0026rdquo; moments and makes PR reviews genuinely meaningful for infrastructure changes.\nFurther Reading\nPractical GitOps Pattern - Repository structure and workflow Kustomize Helmfile diff2html - For visual diff rendering ","date":"January 14, 2025","hero":"/images/default-hero.jpg","permalink":"/posts/gitops-pr-diffs/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\nA common pattern I see promoted is using tools that show you what will change in your cluster \u003cem\u003eat sync time\u003c/em\u003e - after your code is already merged. In my view, this is already too late and goes against GitOps principles. How can Git be the source of truth if there are extra steps between merge and understanding impact?\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll show you how to generate manifest diffs \u003cem\u003eduring PR review\u003c/em\u003e, so reviewers see exactly what will change in the cluster before they approve.\u003c/p\u003e","tags":null,"title":"GitOps PR Diffs: Review What You Deploy"},{"categories":null,"contents":"Sometimes when you\u0026rsquo;re developing or debugging locally you need access to resources that are exposed to your cluster.\nTypically, most organisations use VPN\u0026rsquo;s to enable you to access these resources, but there\u0026rsquo;s a much easier way.\nSocat. The alpine/socat image is perfect for enabling backdoor access to private or internal services that are available to your cluster without having to set up and manage VPN\u0026rsquo;s.\nHow it works is pretty simple. We run a socat pod exposing a service that\u0026rsquo;s viewable by the pod but not by us.\nWe then run a kubectl port-forward to expose the socat forward.\nAt this point we now have access to the private service locally.\nexport PORT=5432 export ADDR=postgres export PODNAME=backdoor kubectl run --restart=Never --image=alpine/socat ${PODNAME} -- -d -d tcp-listen:${PORT},fork,reuseaddr tcp-connect:${ADDR}:${PORT} kubectl wait --for=condition=Ready pod/${PODNAME} kubectl port-forward pod/${PODNAME} ${PORT}:${PORT} You don\u0026rsquo;t need to do use socat As most of you will probably be aware using socat to expose services like this is a bit overkill, you can simply use ExternalName services instead and port-forward that.\nexport PORT=5432 export ADDR=postgres export SERVICE_NAME=backdoor cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: Service apiVersion: v1 metadata: name: ${SERVICE_NAME} spec: type: ExternalName externalName: ${ADDR} EOF kubectl port-forward service/${SERVICE_NAME} ${PORT}:${PORT} ","date":"January 22, 2021","hero":"/posts/socat/hero.jpg","permalink":"/posts/socat/","summary":"\u003cp\u003eSometimes when you\u0026rsquo;re developing or debugging locally you need access to resources that are exposed to your cluster.\u003c/p\u003e\n\u003cp\u003eTypically, most organisations use VPN\u0026rsquo;s to enable you to access these resources, but there\u0026rsquo;s a much easier way.\u003c/p\u003e\n\u003ch2 id=\"socat\"\u003eSocat.\u003c/h2\u003e\n\u003cp\u003eThe alpine/socat image is perfect for enabling backdoor access to private or internal services that are available to\nyour cluster without having to set up and manage VPN\u0026rsquo;s.\u003c/p\u003e\n\u003cp\u003eHow it works is pretty simple. We run a socat pod exposing a service that\u0026rsquo;s viewable by the pod but not by us.\u003c/p\u003e","tags":null,"title":"Using socat to backdoor via kubernetes"}]