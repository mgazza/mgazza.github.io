[{"categories":["Engineering"],"contents":"When I open-sourced the eBPF BNG last month, someone on Hacker News called it \u0026ldquo;vibe coded.\u0026rdquo;\nI understand why. The project moved fast — a working distributed BNG with eBPF/XDP packet processing, DHCP, RADIUS, NAT, PPPoE, BGP, and a coordination service, all open-sourced within weeks. That\u0026rsquo;s suspicious. When something appears quickly, people assume it was thrown together quickly.\nBut speed of implementation isn\u0026rsquo;t the same as absence of design. And using AI tools to write code isn\u0026rsquo;t the same as letting AI design your system.\nThe Backstory I used to work for an ISP startup called Vitrifi. We were building next-generation broadband infrastructure — the platform layer that sits between physical fibre networks and the services running on them. I spent years there working on the problems that a distributed BNG needs to solve: subscriber management, IP allocation, DHCP at scale, RADIUS integration, QoS enforcement, edge deployment models.\nVitrifi went into administration in December 2025. The company didn\u0026rsquo;t make it, but the problems we were working on didn\u0026rsquo;t go away. Neither did the specifications I\u0026rsquo;d written, the architecture decisions I\u0026rsquo;d argued about, or the understanding of why certain approaches work and others don\u0026rsquo;t at ISP scale.\nI can\u0026rsquo;t use any of Vitrifi\u0026rsquo;s code. But I can use what I learned.\nWhen I sat down to build the open-source BNG, I wasn\u0026rsquo;t starting from a blank prompt. I had years of specifications, architecture diagrams, failure mode analyses, and hard-won opinions about how subscriber traffic should flow through an edge network. The system design existed before a single line of code was written.\nWhat Vibe Coding Actually Is Andrej Karpathy coined the term in early 2025. His definition is specific: you describe what you want to a language model, accept what it generates, and keep prompting until it works. You \u0026ldquo;forget that the code even exists.\u0026rdquo; You don\u0026rsquo;t review diffs. You don\u0026rsquo;t understand the implementation. You work around bugs rather than fixing them.\nSimon Willison drew a useful line: \u0026ldquo;I won\u0026rsquo;t commit any code to my repository if I couldn\u0026rsquo;t explain exactly what it does to somebody else.\u0026rdquo;\nThat\u0026rsquo;s the distinction. It\u0026rsquo;s not about whether AI was involved in writing the code. It\u0026rsquo;s about whether a human with relevant expertise is directing the architecture, reviewing the output, and taking responsibility for the result.\nWhat We Actually Did Here\u0026rsquo;s what the development process for the BNG looked like in practice:\n1. Specification first, code second.\nEvery major component started as a design document. How should IP allocation work across distributed BNG nodes? What happens during a network partition? What\u0026rsquo;s the failure mode when a Nexus coordinator goes down? How do you prevent circuit-ID hash collisions in eBPF maps?\nThese aren\u0026rsquo;t questions an LLM can answer. They come from watching real ISP infrastructure break in production and understanding why.\nThe two-tier DHCP design — eBPF fast path for renewals, Go slow path for cache misses — came from understanding that 95%+ of DHCP traffic in a real network is renewals from known subscribers. The decision to allocate IPs at RADIUS authentication time rather than DHCP time came from years of dealing with IP conflict bugs in distributed systems. The offline-first edge design came from working with rural ISPs where backhaul connectivity is unreliable.\nNone of that is something you\u0026rsquo;d arrive at by prompting.\n2. Architecture drives implementation, not the other way around.\nThe system has a clear architecture: central coordination (Nexus) handles only control plane, subscriber traffic stays local at the edge (BNG), state is synchronised via CRDTs, resource allocation uses consistent hashing. This architecture was designed before implementation began — drawn on whiteboards, argued about, refined based on real-world constraints.\nAI tools helped implement that architecture faster. They didn\u0026rsquo;t choose it. When an LLM generates a DHCP server, it\u0026rsquo;ll give you a straightforward single-node implementation. It won\u0026rsquo;t give you a two-tier kernel/userspace split with eBPF map-backed caching and deterministic IP pre-allocation via a hashring. That design comes from domain expertise.\n3. Every line of code is reviewed and understood.\nI can explain what every function in this codebase does, why it exists, and what the alternative approaches were. The eBPF programs are hand-specified — you don\u0026rsquo;t prompt your way to correct XDP packet processing. The Go code uses AI assistance for implementation speed, but every function gets reviewed, tested, and often rewritten.\nWhen the AI generates a RADIUS client, I know whether the authenticator calculation is correct because I\u0026rsquo;ve implemented RADIUS before. When it generates NAT44 logic, I know whether the port allocation strategy will scale because I\u0026rsquo;ve seen the failure modes. The AI accelerates the typing. The engineering judgment is human.\n4. Testing is not optional.\nThe v0.4.0 release includes 3,600+ lines of test code, taking DHCP coverage from 29% to 77%, RADIUS from 23% to 87%, NAT from 24% to 74%. There are 15 integration test scenarios that run real traffic through real eBPF programs in a real Kubernetes cluster.\nVibe coding doesn\u0026rsquo;t produce this. Vibe coding produces something that works for the demo. Engineering produces something that works when the demo breaks.\nThe 4GL Analogy The reaction to AI-assisted development reminds me of every previous abstraction shift in programming.\nWhen developers moved from assembly to C, purists said you couldn\u0026rsquo;t trust a compiler to generate correct machine code. When we moved from C to higher-level languages, people worried about performance and control. When ORMs replaced hand-written SQL, the same arguments appeared.\nEach time, the abstraction layer handled more of the implementation detail. Each time, the domain expertise of the engineer became more important, not less. Nobody argues that using Python instead of assembly makes you less of an engineer. The value moved upstream — from knowing how to write instructions to knowing which instructions to write.\nAI-assisted development is the same pattern. The implementation is increasingly automated. The specification, architecture, testing, and domain knowledge remain human. If anything, they matter more now, because the bottleneck has moved from \u0026ldquo;can we write it fast enough\u0026rdquo; to \u0026ldquo;do we know what to write.\u0026rdquo;\nThe difference between a junior developer prompting ChatGPT and a domain expert using AI tools is the same difference there\u0026rsquo;s always been between someone who can write code and someone who knows what code to write.\nThe Real Risk Isn\u0026rsquo;t AI Tooling The Hacker News commenter who called this \u0026ldquo;vibe coded\u0026rdquo; was pointing at a real concern, just the wrong one. The real risk in infrastructure software isn\u0026rsquo;t whether AI was involved in writing it. It\u0026rsquo;s whether the people building it understand the domain.\nI\u0026rsquo;ve seen beautifully hand-crafted ISP infrastructure that fell over in production because the developer had never worked with real subscriber traffic. I\u0026rsquo;ve seen quick-and-dirty scripts hold up for years because the person who wrote them understood exactly what failure modes mattered.\nThe question to ask about any infrastructure project isn\u0026rsquo;t \u0026ldquo;was this written with AI?\u0026rdquo; It\u0026rsquo;s:\nDoes the architecture reflect real operational experience? Are the failure modes understood and handled? Is there adequate test coverage for the paths that matter? Can the maintainers explain why it works, not just that it works? For this project, the answer to all four is yes. Not because we avoided AI tools, but because we used them within a framework of domain expertise, specifications, and engineering discipline.\nWhat This Means For Open Source Infrastructure There\u0026rsquo;s an uncomfortable truth in the Hacker News thread about commercial viability: traditional ISPs won\u0026rsquo;t adopt this. They have procurement processes, vendor relationships, and support contracts. They need 100% feature parity with their existing Cisco or Juniper BNG before they\u0026rsquo;ll even evaluate an alternative.\nBut there\u0026rsquo;s a growing market of smaller ISPs, WISPs, and altnets who are building on Linux and open-source tooling already. For them, the alternative to this project isn\u0026rsquo;t a six-figure vendor appliance — it\u0026rsquo;s stitching together FreeRADIUS, ISC DHCP, and iptables scripts. An integrated, tested stack built by someone who\u0026rsquo;s spent years working on exactly this problem is a genuine step up.\nThe fact that AI tools accelerated the implementation is a feature, not a bug. It means a small team (or even a single engineer) can build infrastructure that previously required a funded startup with 56 employees. That\u0026rsquo;s the real shift — not replacing engineering judgment, but making it economically viable for smaller players to compete.\nVitrifi burned through £16 million and didn\u0026rsquo;t ship. This project cost nothing and is running. The difference isn\u0026rsquo;t the tools — it\u0026rsquo;s that the specifications already existed in my head before the first line was written.\nLinks:\nKilling the ISP Appliance: An eBPF/XDP Approach to Distributed BNG The Unglamorous Work: Hardening an eBPF BNG for Production bng — The eBPF BNG nexus — Distributed coordination service HN discussion Simon Willison: Not all AI-assisted programming is vibe coding The BNG project is open source and looking for collaborators — particularly ISPs with edge hardware interested in real-world testing. Reach out if that\u0026rsquo;s you.\n","date":"February 14, 2026","hero":"/posts/ai-assisted-engineering/hero.png","permalink":"/posts/ai-assisted-engineering/","summary":"\u003cp\u003eWhen I \u003ca href=\"/posts/ebpf-bng/\"\u003eopen-sourced the eBPF BNG\u003c/a\u003e last month, someone on Hacker News called it \u0026ldquo;vibe coded.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eI understand why. The project moved fast — a working distributed BNG with eBPF/XDP packet processing, DHCP, RADIUS, NAT, PPPoE, BGP, and a coordination service, all open-sourced within weeks. That\u0026rsquo;s suspicious. When something appears quickly, people assume it was thrown together quickly.\u003c/p\u003e\n\u003cp\u003eBut speed of implementation isn\u0026rsquo;t the same as absence of design. And using AI tools to write code isn\u0026rsquo;t the same as letting AI design your system.\u003c/p\u003e","tags":["ebpf","ai","software-engineering","networking","isp","distributed-systems"],"title":"AI Didn't Design This BNG. Experience Did."},{"categories":["Engineering"],"contents":"A month ago I wrote about building an eBPF-accelerated BNG and the infrastructure repo that lets you run it locally. The response was better than I expected — the post hit 94 points on Hacker News and sparked some good discussion.\nIt also sparked some fair criticism.\nOne commenter called the code \u0026ldquo;vibe coded.\u0026rdquo; Another wrote a detailed comment about why distributed BNG has never achieved commercial success, despite attempts by Cisco, Metaswitch, and others. Someone asked about CPU-to-NPU bandwidth in whitebox OLTs. Someone else pointed to 6WIND\u0026rsquo;s commercial DPDK-based BNG as a more production-ready alternative.\nAll valid. The initial post was a prototype — working, but not something you\u0026rsquo;d put in front of real subscribers. This post is about what happened next: the unglamorous work that separates a proof of concept from something you\u0026rsquo;d actually deploy.\nThe Numbers Since the initial release, across three repositories (BNG, Nexus, Infra):\n124 issues closed 88 pull requests merged 81,000 lines changed 4 releases (v0.1 → v0.4.0) Most of that wasn\u0026rsquo;t new features. It was testing, security hardening, failure injection, and fixing the kind of bugs that only appear when you start trying to break things on purpose.\nTest Coverage: The Biggest Gap, Now Filled The most damning thing about the initial release was the test coverage. You can\u0026rsquo;t call something production-ready when the DHCP server — the core of a BNG — has 29% test coverage.\nSo we fixed it:\nPackage Before After Change DHCP 29.9% 76.9% +47pp RADIUS 23.0% 87.1% +64pp NAT 23.8% 74.4% +50pp Subscriber 86.9% 100% +13pp eBPF Loader 39.3% 45.6% +6pp That\u0026rsquo;s 3,600+ lines of new test code covering the paths that actually matter: DHCP message handling, RADIUS authentication and accounting, NAT port allocation and hairpinning, CoA processing, rate limiting, and concurrent access patterns.\nThe eBPF loader is still at 45% — eBPF C code is genuinely hard to unit test. The coverage there comes from the 15 integration test scenarios that exercise the full stack end-to-end rather than mocking the kernel.\n15 Integration Test Scenarios The infra repo now ships with 15 test configurations that run in k3d, covering every major BNG function:\ntilt up e2e # Full DHCP → BNG → Nexus → allocation flow tilt up pppoe-test # PPPoE lifecycle: PADI → PADS → LCP → Auth → IPCP tilt up nat-test # NAT44/CGNAT: port blocks, hairpinning, EIM/EIF tilt up ipv6-test # SLAAC + DHCPv6 (IA_NA and IA_PD) tilt up qos-test # Per-subscriber TC rate limiting tilt up bgp-test # BGP session + BFD + route injection tilt up ha-p2p-test # Active/standby failover with P2P state sync tilt up ha-nexus-test # HA pair with shared Nexus tilt up failure-test # Nexus failure, BNG failover, split-brain recovery tilt up wifi-test # TTL-based short-lived allocations tilt up radius-time-test # IP pre-allocation at RADIUS auth time tilt up peer-pool-test # Hashring coordination without Nexus tilt up walled-garden-test # Captive portal for unauth\u0026#39;d subscribers tilt up blaster-test # BNG Blaster traffic generation Each one runs a real BNG with real DHCP traffic in a real Kubernetes cluster. Not mocks, not simulations — actual packets flowing through actual eBPF programs. When any of these break, we know about it before a subscriber would.\nSecurity Hardening The initial release had basic security — mTLS for Nexus communication, PSK authentication. But \u0026ldquo;basic\u0026rdquo; isn\u0026rsquo;t enough when you\u0026rsquo;re handling subscriber traffic. Here\u0026rsquo;s what v0.4.0 added:\nRADIUS Rate Limiting A BNG without rate limiting on RADIUS is an amplification vector. We added per-server token-bucket rate limiting:\n// Default: 1000 req/s sustained, burst of 100 config := RateLimitConfig{ Rate: 1000, Burst: 100, } Both authentication and accounting requests go through the limiter. If a server is being hammered, requests get cleanly rejected rather than forwarded.\nCAP_BPF Capability Verification Before v0.4.0, running the BNG without the right Linux capabilities produced a cryptic kernel EPERM error. Now it checks upfront:\n$ ./bng run --interface eth1 Error: eBPF requires CAP_BPF (Linux 5.8+) or CAP_SYS_ADMIN. Current capabilities: CAP_NET_ADMIN, CAP_NET_RAW Run with: sudo setcap cap_bpf+ep ./bng Small thing, but it\u0026rsquo;s the difference between a 30-second fix and a 30-minute debugging session.\nSecrets Out of Process Lists RADIUS shared secrets passed via CLI flags are visible in ps aux. New flags read secrets from files instead:\n# Before (secret visible in ps output) ./bng run --radius-secret \u0026#34;my-shared-secret\u0026#34; # After echo \u0026#34;my-shared-secret\u0026#34; \u0026gt; /etc/bng/radius-secret ./bng run --radius-secret-file /etc/bng/radius-secret Other Security Fixes PPPoE password zeroing — credentials are wiped from memory after authentication completes Option 82 buffer alignment — fixed buffer size constants to prevent potential overflows Data race fixes — all shared counters moved to atomic operations (caught by -race flag in CI) New Features That Matter Not everything was hardening. Some features were needed to make the architecture viable for real deployments.\nBGP Controller Integration A BNG needs to announce subscriber routes to the upstream network. v0.4.0 wires in a BGP controller with BFD for fast failure detection:\n./bng run \\ --interface eth1 \\ --bgp-enabled \\ --bgp-local-as 65001 \\ --bgp-neighbors \u0026#34;10.0.0.1:65000\u0026#34; \\ --bgp-bfd-enabled When a subscriber session comes up, the route is injected. When it goes down, the route is withdrawn. BFD gives sub-second failure detection — important when you\u0026rsquo;re distributing routing across hundreds of edge sites.\nHA with TLS/mTLS State Sync The HA implementation from v0.3 worked, but peer state sync was unencrypted. v0.4.0 adds full TLS and mutual TLS for the SSE-based state replication between active/standby BNG pairs. In a deployment where BNG peers might communicate over untrusted networks, this matters.\nCircuit-ID Collision Detection Circuit-ID is how we identify subscriber ports — but it\u0026rsquo;s hashed to a 32-bit key for eBPF map lookups. Hash collisions are rare but catastrophic (two subscribers sharing an identity). v0.4.0 detects collisions at load time and exports Prometheus metrics:\nbng_circuit_id_collisions_total # Total collisions detected bng_circuit_id_collision_rate # Current collision rate At 2,000 subscribers per OLT, the collision probability is negligible. But monitoring it means you\u0026rsquo;ll know before it becomes a problem.\nDynamic Pool Configuration from Nexus Previously, gateway and DNS settings were hardcoded per BNG instance. Now they\u0026rsquo;re pulled from Nexus pool configuration, so a central change propagates to all edge sites. One less thing to configure per-site.\nWhat\u0026rsquo;s Still Missing Honesty about gaps is more useful than pretending they don\u0026rsquo;t exist.\nReal hardware validation. Everything runs in k3d on simulated interfaces. The architecture is sound, the code is tested, but nobody has run this on an actual OLT with real subscriber traffic yet. That\u0026rsquo;s the next step, and it\u0026rsquo;s the one I can\u0026rsquo;t do alone.\neBPF fast path coverage. The Go code is well-tested. The eBPF C programs are validated through integration tests but don\u0026rsquo;t have unit-level coverage. eBPF testing tooling is still immature — there\u0026rsquo;s no good equivalent of go test for BPF programs.\nScale testing under load. The BNG Blaster tests generate traffic, but we haven\u0026rsquo;t done sustained load testing at 1,500+ concurrent subscribers. The architecture should handle it (eBPF maps scale well), but \u0026ldquo;should\u0026rdquo; isn\u0026rsquo;t \u0026ldquo;proven.\u0026rdquo;\nOperational tooling. There\u0026rsquo;s Prometheus metrics and a CLI, but no management UI, no alerting templates, no runbooks. The kind of operational tooling that on-call engineers expect.\nThe Market Question The most thoughtful HN comment was about why distributed BNG hasn\u0026rsquo;t succeeded commercially. The barriers aren\u0026rsquo;t technical — they\u0026rsquo;re organisational. ISPs have procurement processes, vendor relationships, regulatory requirements, and staff whose expertise is in existing platforms. You don\u0026rsquo;t displace a Cisco ASR9000 with a GitHub repo.\nI don\u0026rsquo;t think that\u0026rsquo;s the target market though. The opportunity is with smaller ISPs, WISPs, and altnets who:\nCan\u0026rsquo;t afford six-figure BNG appliances Run on white-box hardware already (MikroTik, Ubiquiti, or bare Linux) Have engineering teams comfortable with Linux and containers Are building new networks rather than migrating legacy ones For them, the alternative to this project isn\u0026rsquo;t a Cisco BNG — it\u0026rsquo;s bolting together FreeRADIUS, ISC DHCP, and iptables scripts. An integrated, tested, eBPF-accelerated stack is a genuine improvement over that.\nWhat Changed, Really If you read the original post a month ago and thought \u0026ldquo;interesting prototype, but I wouldn\u0026rsquo;t run it\u0026rdquo; — that was the right reaction. Here\u0026rsquo;s what\u0026rsquo;s different now:\nAspect January (v0.1) February (v0.4.0) Test coverage Minimal 76-100% across core packages Security Basic mTLS Rate limiting, capability checks, secret management, memory zeroing HA Not implemented Active/standby with encrypted state sync Routing Static BGP with BFD Test scenarios 4 demos 15 integration tests Data races Present Fixed (atomic ops, CI race detection) IPv6 Not implemented SLAAC + DHCPv6 NAT Basic Full CGNAT with hairpinning, EIM/EIF, ALG PPPoE Basic Full lifecycle with proper auth It\u0026rsquo;s not production-ready in the sense that you can deploy it tomorrow with no risk. But it\u0026rsquo;s production-ready in the sense that the engineering work has been done to make that deployment possible — the testing, the security hardening, the failure handling. What\u0026rsquo;s missing now is validation on real hardware with real traffic.\nTry It If you\u0026rsquo;re running edge infrastructure and want to test this on real OLT hardware, I want to hear from you. The entire stack runs locally in 15 minutes:\ngit clone --recurse-submodules git@github.com:codelaboratoryltd/bng-edge-infra.git cd bng-edge-infra ./scripts/init.sh tilt up e2e Links:\nbng — The eBPF BNG (v0.4.0) nexus — Distributed coordination service (v0.1.0) bng-edge-infra — Infrastructure and test harnesses (v0.1.0) Original post: Killing the ISP Appliance Infra post: From Zero to eBPF BNG in 15 Minutes HN discussion If you\u0026rsquo;re an ISP, WISP, or altnet with edge hardware and want to collaborate on real-world testing, reach out. That\u0026rsquo;s the one thing I can\u0026rsquo;t do from a k3d cluster.\n","date":"February 14, 2026","hero":"/posts/ebpf-bng-production/hero.png","permalink":"/posts/ebpf-bng-production/","summary":"\u003cp\u003eA month ago I wrote about \u003ca href=\"/posts/ebpf-bng/\"\u003ebuilding an eBPF-accelerated BNG\u003c/a\u003e and the \u003ca href=\"/posts/ebpf-bng-infra/\"\u003einfrastructure repo\u003c/a\u003e that lets you run it locally. The response was better than I expected — the post hit 94 points on Hacker News and sparked some good discussion.\u003c/p\u003e\n\u003cp\u003eIt also sparked some fair criticism.\u003c/p\u003e\n\u003cp\u003eOne commenter called the code \u0026ldquo;vibe coded.\u0026rdquo; Another wrote a detailed comment about why distributed BNG has never achieved commercial success, despite attempts by Cisco, Metaswitch, and others. Someone asked about CPU-to-NPU bandwidth in whitebox OLTs. Someone else pointed to 6WIND\u0026rsquo;s commercial DPDK-based BNG as a more production-ready alternative.\u003c/p\u003e","tags":["ebpf","xdp","networking","isp","distributed-systems","go","linux","testing","security"],"title":"The Unglamorous Work: Hardening an eBPF BNG for Production"},{"categories":["Engineering"],"contents":"Last week I open-sourced the eBPF BNG itself. The response was great, but the most common question was: \u0026ldquo;How do I actually run this thing?\u0026rdquo;\nFair question. The BNG repo has a Dockerfile and some example configs, but spinning up a distributed system with multiple components, observability, and realistic test traffic isn\u0026rsquo;t trivial. That\u0026rsquo;s the hard part of infrastructure - not writing the code, but figuring out how to deploy it, test it, and debug it when things break.\nSo this week I\u0026rsquo;m open-sourcing the infrastructure repo: bng-edge-infra. One command gets you a complete eBPF BNG stack running locally.\nThe Problem: \u0026ldquo;Works On My Machine\u0026rdquo; Doesn\u0026rsquo;t Scale When I was building the BNG, I spent more time on deployment tooling than I\u0026rsquo;d like to admit. The challenges:\nMultiple components with dependencies - BNG needs Nexus for IP allocation. Nexus nodes need to discover each other. Everything needs networking configured correctly.\nNo hardware to test on - Real OLTs cost thousands. You can\u0026rsquo;t ask contributors to buy one to test a PR.\nDistributed systems are hard to debug - When a DHCP request fails, is it the eBPF program? The userspace fallback? Nexus? Network policy? You need visibility.\nThe \u0026ldquo;getting started\u0026rdquo; cliff - Vendor docs assume you already understand the system. They show you config snippets, not working examples.\nThe solution: a GitOps repo that codifies everything - from cluster creation to traffic testing - in a reproducible, one-command experience.\nWhat\u0026rsquo;s In The Repo bng-edge-infra/ ├── clusters/ │ ├── local-dev/ # k3d cluster config │ ├── staging/ # Staging Flux manifests │ └── production/ # Production Flux manifests ├── components/ │ ├── base/ # Reusable base components │ │ ├── bng/ # BNG deployment template │ │ ├── nexus/ # Nexus StatefulSet template │ │ ├── nexus-p2p/ # Nexus P2P cluster template │ │ └── rendezvous-server/ # P2P discovery server │ ├── demos/ # Demo overlays │ │ ├── standalone/ # Demo A │ │ ├── single/ # Demo B │ │ ├── p2p-cluster/ # Demo C │ │ └── distributed/ # Demo D │ ├── bngblaster/ # Traffic generator │ └── ... # Test components ├── charts/ # Infrastructure (Cilium, Prometheus, Grafana) ├── src/ │ ├── bng/ # SUBMODULE: BNG source │ └── nexus/ # SUBMODULE: Nexus source └── Tiltfile # The magic The key insight is treating the entire deployment experience as code. Not just the Kubernetes manifests, but the cluster creation, the infrastructure setup, the port forwarding, and the test harnesses.\nFour Demo Configurations The repo ships with four progressively complex deployment modes. Each one teaches you something different about the architecture:\n┌─────────────────────────────────────────────────────────────────────┐ │ Demo A: Standalone BNG │ │ ┌─────────┐ │ │ │ BNG │ ← Single BNG, local IP pool, no external deps │ │ └─────────┘ │ │ Good for: Understanding BNG internals, eBPF debugging │ └─────────────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────────────┐ │ Demo B: Single Integration │ │ ┌─────────┐ ┌─────────┐ │ │ │ Nexus │ ←──→ │ BNG │ ← BNG gets IPs from Nexus │ │ └─────────┘ └─────────┘ │ │ Good for: Testing the integration, API exploration │ └─────────────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────────────┐ │ Demo C: Nexus P2P Cluster │ │ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ │ │ Nexus-0 │ ↔ │ Nexus-1 │ ↔ │ Nexus-2 │ ← CRDT sync via libp2p │ │ └─────────┘ └─────────┘ └─────────┘ │ │ Good for: Testing distributed state, partition tolerance │ └─────────────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────────────┐ │ Demo D: Full Distributed │ │ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ │ │ Nexus-0 │ ↔ │ Nexus-1 │ ↔ │ Nexus-2 │ │ │ └────┬────┘ └────┬────┘ └────┬────┘ │ │ │ │ │ │ │ └──────┬──────┴──────┬──────┘ │ │ ▼ ▼ │ │ ┌─────────┐ ┌─────────┐ │ │ │ BNG-0 │ │ BNG-1 │ ← Multiple BNGs, shared state │ │ └─────────┘ └─────────┘ │ │ Good for: Production-like testing, HA validation │ └─────────────────────────────────────────────────────────────────────┘ You can run any specific demo or all of them at once:\ntilt up # Shows available groups tilt up demo-a # Just standalone BNG tilt up demo-d # Just full distributed tilt up demo-a demo-b # Multiple demos tilt up all # Everything The One-Command Experience Prerequisites You need Docker and about 8GB of RAM. Then install the tools:\nmacOS:\nbrew install k3d kubectl tilt-dev/tap/tilt Linux:\n# k3d curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash # kubectl curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ # tilt curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh | bash Clone and Run git clone --recurse-submodules git@github.com:codelaboratoryltd/bng-edge-infra.git cd bng-edge-infra ./scripts/init.sh # Create k3d cluster (first time only) tilt up demo-a # Run standalone BNG demo That\u0026rsquo;s it. Go make a coffee.\nWhat Happens Under The Hood The setup is split into two steps:\nFirst, ./scripts/init.sh creates the k3d cluster:\nA local Kubernetes cluster named bng-edge spins up Flannel is disabled (we use Cilium instead) A local Docker registry is created at localhost:5555 Then, tilt up \u0026lt;group\u0026gt; does the rest:\nCilium installation - The eBPF-based CNI is deployed via kustomize, enabling Hubble observability.\nDocker builds - BNG and Nexus images are built from the submodules and pushed to the local registry.\nDemo deployment - The selected demo components are deployed via kustomize. Each demo gets its own namespace (demo-standalone, demo-single, demo-p2p, demo-distributed).\nPort forwarding - Services are exposed to localhost automatically.\nLive reload - File changes trigger automatic rebuilds and redeployments.\nAfter a few minutes, you\u0026rsquo;ll have:\nService URL Notes Tilt UI http://localhost:10350 Always available BNG API (Demo A) http://localhost:8080 tilt up demo-a Nexus API (Demo B) http://localhost:9001 tilt up demo-b BNG API (Demo D) http://localhost:8083 tilt up demo-d Hubble UI http://localhost:12000 tilt up \u0026lt;demo\u0026gt; infra Prometheus http://localhost:9090 tilt up \u0026lt;demo\u0026gt; infra Grafana http://localhost:3000 tilt up \u0026lt;demo\u0026gt; infra Testing Without Real Hardware One of the biggest challenges with BNG development is that you normally need real subscriber hardware to generate DHCP/PPPoE traffic. We solve this with built-in test harnesses.\nVerification Buttons Each demo has a \u0026ldquo;verify\u0026rdquo; button in the Tilt UI. Click it to run a quick health check:\nDemo A: Queries the BNG sessions API Demo B: Creates a pool in Nexus, allocates an IP, verifies the integration Demo C: Creates a pool on Nexus-0, waits for CRDT sync, verifies it appears on Nexus-1 Demo D: Full integration test across the distributed cluster BNG Blaster For serious traffic testing, we include BNG Blaster - an open-source traffic generator designed for BNG testing:\ntilt up blaster This gives you pre-configured test scenarios:\nIPoE session establishment PPPoE session establishment DHCP stress testing Realistic DHCP Testing For quick DHCP validation without BNG Blaster\u0026rsquo;s complexity, there\u0026rsquo;s a lightweight test harness:\ntilt up blaster-test This spins up a BNG with a sidecar container that can generate real DHCP traffic over a veth pair. The Tilt UI has buttons for:\nSingle DHCP request - One udhcpc request to verify basic functionality Stress test - 10 virtual clients requesting IPs concurrently Check sessions - Query the BNG API to see allocated sessions Example output from the stress test:\n=== Multi-Client DHCP Stress Test === Creating 10 virtual clients... Running DHCP requests... Client 1: 10.100.0.2/24 Client 2: 10.100.0.3/24 Client 3: 10.100.0.4/24 ... === Results === Successful: 10 / 10 Failed: 0 Duration: 3s Rate: 3 sessions/sec Observability Built In Debugging distributed systems without observability is misery. The repo comes with a full stack pre-configured.\nCilium + Hubble We use Cilium as the CNI instead of the default Flannel. This gives us:\neBPF-based networking - Fitting, given the BNG itself uses eBPF Network policies - If you want to test isolation Hubble - Real-time network flow visibility The Hubble UI (http://localhost:12000) shows you every packet flowing through the cluster. When a DHCP request fails, you can see exactly where it got dropped.\n# Or use the CLI hubble observe --namespace demo-distributed hubble observe --protocol udp --port 67 # DHCP traffic only Prometheus + Grafana Both BNG and Nexus export Prometheus metrics. The repo includes:\nPre-configured scrape configs Basic dashboards for BNG session counts, DHCP latency, Nexus allocation rates Grafana is available at http://localhost:3000 (admin/admin).\nWhy This Matters When you\u0026rsquo;re debugging \u0026ldquo;DHCP isn\u0026rsquo;t working\u0026rdquo;, the question is always where in the stack it\u0026rsquo;s failing:\nIs the packet reaching the BNG pod? → Hubble Is the eBPF fast path matching? → BNG metrics (dhcp_fastpath_hits) Is the slow path timing out on Nexus? → Nexus metrics + traces Is the response getting back to the client? → Hubble again Having all of this in one place, with one command, makes the difference between debugging for hours and debugging for minutes.\nTaking It To Production The local-dev setup is designed for experimentation, but the repo structure supports real deployments.\nStaging and Production Clusters The clusters/staging/ and clusters/production/ directories are set up for FluxCD:\nclusters/ ├── local-dev/ │ └── k3d-config.yaml ├── staging/ │ ├── flux-system/ # Flux bootstrap │ ├── infrastructure/ # Cilium, monitoring │ └── apps/ # BNG, Nexus └── production/ ├── flux-system/ ├── infrastructure/ └── apps/ Point Flux at the appropriate directory, and it\u0026rsquo;ll keep your cluster in sync with Git. The same manifests that work locally work in production - that\u0026rsquo;s the point of GitOps.\nExtending For Your Deployment The demo configurations are intentionally simple. For a real deployment, you\u0026rsquo;d add:\nRADIUS integration - The BNG supports FreeRADIUS. Add your RADIUS server config. Real IP pools - Replace the 10.x.x.x demo pools with your actual allocations. TLS everywhere - The demos use HTTP for simplicity. Production should use mTLS. Persistent storage - Nexus supports persistent state. Configure a StorageClass. Network interfaces - The demos use standard CNI networking. Real OLTs need interface configuration for subscriber-facing ports. The Bigger Picture This repo represents a philosophy: infrastructure should be runnable by anyone.\nToo often, infrastructure projects assume you\u0026rsquo;ll figure out deployment yourself. They give you a binary and some config flags and wish you luck. That\u0026rsquo;s fine for simple tools, but distributed systems are different. The deployment is the product.\nBy open-sourcing not just the BNG code but the entire deployment experience, we\u0026rsquo;re saying: this is how we think it should work. Clone it, run it, break it, improve it.\nIf you\u0026rsquo;re building ISP infrastructure, evaluating the architecture, or just curious about GitOps patterns for edge systems - this is your starting point.\nTry it:\ngit clone --recurse-submodules git@github.com:codelaboratoryltd/bng-edge-infra.git cd bng-edge-infra ./scripts/init.sh # Create k3d cluster tilt up demo-a # Run standalone BNG demo Links:\nbng-edge-infra - This repo bng - The eBPF BNG itself nexus - Distributed coordination service Questions? Feedback? Found a bug? Open an issue or reach out.\n","date":"January 24, 2026","hero":"/posts/ebpf-bng-infra/hero.png","permalink":"/posts/ebpf-bng-infra/","summary":"\u003cp\u003eLast week I \u003ca href=\"/posts/ebpf-bng/\"\u003eopen-sourced the eBPF BNG\u003c/a\u003e itself. The response was great, but the most common question was: \u003cem\u003e\u0026ldquo;How do I actually run this thing?\u0026rdquo;\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eFair question. The BNG repo has a Dockerfile and some example configs, but spinning up a distributed system with multiple components, observability, and realistic test traffic isn\u0026rsquo;t trivial. That\u0026rsquo;s the hard part of infrastructure - not writing the code, but figuring out how to deploy it, test it, and debug it when things break.\u003c/p\u003e","tags":["ebpf","gitops","kubernetes","tilt","k3d","isp","networking"],"title":"From Zero to eBPF BNG in 15 Minutes: The GitOps Deployment Repo"},{"categories":["Engineering"],"contents":"I used to work for an ISP startup that was building next-generation infrastructure. The company didn\u0026rsquo;t make it, but the problems we were trying to solve stuck with me. So I spent a few weeks building what we never got to: an open-source, eBPF-accelerated BNG that runs directly on OLT hardware.\nThis post explains the architecture and why I think it\u0026rsquo;s the future of ISP edge infrastructure.\nThe Problem: Centralised BNG is a Bottleneck Traditional ISP architecture looks like this:\nCustomer → ONT → OLT → [BNG Appliance] → Internet ↑ Single point of failure Expensive proprietary hardware All subscriber traffic flows through here Every subscriber\u0026rsquo;s traffic - DHCP, authentication, NAT, QoS - flows through a central BNG appliance. These boxes cost six figures, require vendor support contracts, and create a single point of failure. When they go down, everyone goes down.\nThe industry\u0026rsquo;s answer has been to buy bigger boxes with more redundancy. But what if we flipped the model entirely?\nThe Idea: Distribute the BNG to the Edge What if, instead of funneling all traffic through a central appliance, we ran BNG functions directly on the OLT hardware at each edge site?\nCustomer → ONT → OLT(+BNG) → Internet ↑ Subscriber traffic stays LOCAL No central bottleneck Each site operates independently This isn\u0026rsquo;t a new idea - it\u0026rsquo;s essentially what hyperscalers do with their edge infrastructure. But ISPs have been slow to adopt it because:\nTraditional BNG software assumes a central deployment State management (IP allocations, sessions) is hard to distribute Performance requirements seemed to need specialised hardware The key insight is that modern Linux with eBPF/XDP can handle ISP-scale packet processing on commodity hardware.\nWhy eBPF/XDP, Not VPP? When I started this project, I evaluated two approaches:\nVPP (Vector Packet Processing) - The industry darling for high-performance networking. Used in production by big telcos. Handles 100+ Gbps easily.\neBPF/XDP - Linux kernel\u0026rsquo;s programmable packet processing. Lower peak throughput, but much simpler operations.\nFor edge deployment (10-40 Gbps per OLT), I chose eBPF/XDP:\nAspect eBPF/XDP VPP Performance 10-40 Gbps ✓ 100+ Gbps (overkill) Deployment Standard Linux kernel DPDK, hugepages, dedicated NICs Operations systemd service Complex dedicated setup Debugging tcpdump, bpftool, perf Custom tools Learning curve Steep but well-documented Very steep, less documentation VPP is the right choice for core aggregation. But for edge sites? eBPF/XDP is simpler and sufficient.\nThe Architecture Here\u0026rsquo;s what I built:\n┌─────────────────────────────────────────────────────────────┐ │ CENTRAL (Kubernetes) │ │ Nexus: CRDT state sync, hashring IP allocation │ │ (Control plane only - NO subscriber traffic) │ └──────────────────────────┬──────────────────────────────────┘ │ Config sync, metrics ┌─────────────────┼─────────────────┐ ▼ ▼ ▼ ┌───────────┐ ┌───────────┐ ┌───────────┐ │ OLT-BNG 1 │ │ OLT-BNG 2 │ │ OLT-BNG N │ │ eBPF/XDP │ │ eBPF/XDP │ │ eBPF/XDP │ │ 1500 subs │ │ 2000 subs │ │ 1800 subs │ └─────┬─────┘ └─────┬─────┘ └─────┬─────┘ │ │ │ Traffic LOCAL Traffic LOCAL Traffic LOCAL ↓ ↓ ↓ ISP PE ISP PE ISP PE Key principle: Subscriber traffic never touches central infrastructure. The central Nexus server only handles control plane operations - config distribution, IP allocation coordination, monitoring.\nTwo-Tier DHCP: Fast Path + Slow Path The performance-critical insight is that most DHCP operations are renewals from known subscribers. We can handle these entirely in the kernel:\nDHCP Request arrives │ ▼ ┌───────────────────────────────────────────────────────┐ │ XDP Fast Path (Kernel) │ │ │ │ 1. Parse Ethernet → IP → UDP → DHCP │ │ 2. Extract client MAC │ │ 3. Lookup MAC in eBPF subscriber_pools map │ │ │ │ CACHE HIT? │ │ ├─ YES: Generate DHCP ACK in kernel │ │ │ Return XDP_TX (~10μs latency) │ │ └─ NO: Return XDP_PASS → userspace │ └───────────────────────────────────────────────────────┘ │ XDP_PASS (cache miss) ▼ ┌───────────────────────────────────────────────────────┐ │ Go Slow Path (Userspace) │ │ │ │ 1. Lookup subscriber in Nexus cache │ │ 2. Get pre-allocated IP from subscriber record │ │ 3. Update eBPF cache for future fast path hits │ │ 4. Send DHCP response │ └───────────────────────────────────────────────────────┘ Results:\nFast path: ~10μs latency, 45,000+ requests/sec Slow path: ~10ms latency, 5,000 requests/sec Cache hit rate after warmup: \u0026gt;95% IP Allocation: Hashring at RADIUS Time Here\u0026rsquo;s a design decision that simplified everything: IP allocation happens at RADIUS authentication time, not DHCP time.\n1. Subscriber authenticates via RADIUS 2. RADIUS success → Nexus allocates IP from hashring (deterministic) 3. IP stored in subscriber record 4. DHCP is just a READ operation (lookup pre-allocated IP) This means:\nNo IP conflicts between distributed BNG nodes DHCP fast path can run entirely in eBPF (no userspace allocation decisions) Subscribers get the same IP every time (hashring determinism) Offline-First Edge Operation What happens when an edge site loses connectivity to central Nexus?\nKeeps working:\nExisting subscriber sessions (cached in eBPF maps) DHCP lease renewals NAT translations QoS enforcement Degraded:\nNew subscriber authentication (no RADIUS) New IP allocations (falls back to local pool) Config updates (queued until reconnect) The edge sites are designed to be autonomous. Central coordination is nice to have, not required.\nThe Implementation The BNG is a single Go binary with embedded eBPF programs:\nbng/ ├── cmd/bng/ # Main binary ├── pkg/ │ ├── ebpf/ # eBPF loader and map management │ ├── dhcp/ # DHCP slow path server │ ├── nexus/ # Central coordination client │ ├── radius/ # RADIUS client │ ├── qos/ # QoS/rate limiting │ ├── nat/ # NAT44/CGNAT │ ├── pppoe/ # PPPoE server │ ├── routing/ # BGP/FRR integration │ └── metrics/ # Prometheus metrics ├── bpf/ │ ├── dhcp_fastpath.c # XDP DHCP fast path │ ├── qos_ratelimit.c # TC QoS eBPF │ ├── nat44.c # TC NAT eBPF │ └── antispoof.c # TC anti-spoofing Running it:\n# Standalone mode (local IP pool) sudo ./bng run \\ --interface eth1 \\ --pool-network 10.0.1.0/24 \\ --pool-gateway 10.0.1.1 # Production mode (with Nexus coordination) sudo ./bng run \\ --interface eth1 \\ --nexus-url http://nexus.internal:9000 \\ --radius-enabled \\ --radius-servers radius.isp.com:1812 Hardware: White-Box OLTs This runs on any Linux box with a modern kernel (5.10+), but the target is white-box OLTs like the Radisys RLT-1600G:\n16 GPON/XGS-PON ports Runs Debian Linux ~$7,400 USD (vs six figures for traditional BNG) 1,500-2,000 subscribers per unit The same approach works with any OLT that runs Linux and exposes its network interfaces to the OS.\nWhat\u0026rsquo;s Next The code is working but not production-ready. Missing pieces:\nDevice authentication - TPM attestation or similar to prevent rogue OLT-BNG devices IPv6 support - DHCPv6 and SLAAC Full RADIUS accounting - Currently basic Management UI - Currently CLI and Prometheus metrics only I\u0026rsquo;m considering open-sourcing the entire thing. The BNG market is dominated by expensive proprietary solutions, and there\u0026rsquo;s no good open-source alternative. Maybe there should be.\nThe Bigger Picture Traditional ISP infrastructure was designed when compute was expensive and networks were slow. Centralised appliances made sense when you needed specialised hardware for packet processing.\nBut compute is cheap now, and eBPF lets us do packet processing in the Linux kernel at line rate. The economics have shifted - it\u0026rsquo;s now cheaper to distribute the BNG to hundreds of edge sites than to build a few massive central boxes.\nThis isn\u0026rsquo;t just about saving money. Distributed architecture is more resilient (no single point of failure), lower latency (traffic stays local), and operationally simpler (it\u0026rsquo;s just Linux).\nThe hyperscalers figured this out years ago. ISPs are slowly catching up.\nInterested in this approach? The code is at github.com/codelaboratoryltd/bng and github.com/codelaboratoryltd/nexus. I\u0026rsquo;d love to hear from anyone working on similar problems in the ISP/altnet space.\nIf you\u0026rsquo;re building ISP infrastructure and want to chat about eBPF, distributed systems, or why vendor BNG appliances are a racket, reach out.\n","date":"January 16, 2026","hero":"/posts/ebpf-bng/hero.png","permalink":"/posts/ebpf-bng/","summary":"\u003cp\u003eI used to work for an ISP startup that was building next-generation infrastructure. The company didn\u0026rsquo;t make it, but the problems we were trying to solve stuck with me. So I spent a few weeks building what we never got to: an open-source, eBPF-accelerated BNG that runs directly on OLT hardware.\u003c/p\u003e\n\u003cp\u003eThis post explains the architecture and why I think it\u0026rsquo;s the future of ISP edge infrastructure.\u003c/p\u003e\n\u003ch2 id=\"the-problem-centralised-bng-is-a-bottleneck\"\u003eThe Problem: Centralised BNG is a Bottleneck\u003c/h2\u003e\n\u003cp\u003eTraditional ISP architecture looks like this:\u003c/p\u003e","tags":["ebpf","xdp","networking","isp","distributed-systems","go","linux"],"title":"Killing the ISP Appliance: An eBPF/XDP Approach to Distributed BNG"},{"categories":null,"contents":"Introduction\nIf you’ve spent any time working with Kubernetes, you’ve probably heard of GitOps -a methodology that treats Git as the source of truth for defining and operating infrastructure and applications. In this post, I’ll walk you through a GitOps setup that uses a hierarchical folder structure, combining Helm, Helmfile, and Kustomize to give you robust, testable, and scalable deployments. We’ll also see how tools like Flux and Tilt fit into the workflow, enabling both automated deployments and seamless local development.\nWhy GitOps? Before we dive into the specifics, let’s revisit what GitOps brings to the table:\nVersion Control: Every change to your Kubernetes configurations is committed to Git, providing an audit trail and easy rollbacks. Single Source of Truth: Teams can rely on the repo as the canonical description of what’s running in each cluster. Automation: Changes in Git trigger updates to your clusters, reducing manual operations and ensuring consistency. This post assumes you\u0026rsquo;re already sold on GitOps and are looking for a tangible organisational pattern. Let\u0026rsquo;s jump in.\nThe Repository Structure Our GitOps repository is divided into four main folders -plus a special src directory for source code and a Tiltfile for local dev. Here’s a quick overview:\n. ├── helm-charts ├── components ├── groups ├── clusters ├── src └── Tiltfile 1. Helm Charts Purpose: This directory stores all Helm charts -whether first-party or third-party dependencies -that form the foundation of your Kubernetes services. Workflow: Render: Helm charts are templated to disk. Include in Kustomize: You use Kustomize to ingest those templates. Manage with Helmfile: Helmfile can orchestrate multiple Helm releases, ensuring everything is installed/updated consistently. This approach decouples the raw Helm charts from the environment-specific overlays, making it easier to plug them into different clusters in a standardized way.\n2. Components Purpose: Store Kubernetes manifests that are not part of Helm charts. This could include CRDs (Custom Resource Definitions), operator manifests, or any other resources you want to keep separate. Usage: Directly reference these components in your cluster definitions or in a higher-level grouping concept (more on that next). 3. Groups Purpose: Group related services and configurations together under a single overlay. For example, a monitoring group might include Prometheus, Grafana, and other supporting components. Hierarchy: Groups can reference other groups, enabling layering. A dev group might inherit from a default group, adding environment-specific patches for development clusters (e.g., less resource allocation, debug logging). 4. Clusters Purpose: Each cluster folder describes exactly what should be running on that cluster, pulling in components and groups as needed. Structure: Each cluster has its own folder, which Flux (or another GitOps tool like Argo CD) monitors. Subfolders often map to namespaces or functional areas. Environment-specific customisations, such as image overrides or domain-specific settings, also live here. Benefits: This design ensures that each cluster references only the resources it needs, with any environment-specific overrides captured in a single place. 5. src (Git Submodules) Purpose: Each application or service your team develops has its own dedicated repo, added to this GitOps repo as a submodule. Motivation: Separating source code lifecycles from infrastructure while still keeping them in close proximity. Each service can evolve at its own pace (with separate versioning and pull requests). When you’re ready to deploy, you update references in the GitOps repo to point to the correct version or commit. 6. Tilt for Local Development Tiltfile: A single Tiltfile at the root of your GitOps repo configures local Kubernetes development using Tilt and k3d. Realtime Feedback: Tilt builds Docker images locally as you code and pushes them into your k3d cluster. You can check out feature branches across multiple submodules and test them all together in a local environment. Developer Happiness: This local dev approach drastically shortens the feedback loop, letting you iterate faster than if you had to push and wait for a remote pipeline to run. CI/CD Flow Now that we’ve broken down the structure, let’s see how changes flow from a developer’s pull request to a cluster.\n1. Pull Request → Image Build Trigger: A developer creates or updates a pull request in the src project repository. Automation: A CI pipeline (e.g., GitHub Actions, Jenkins, GitLab CI) builds a Docker image for the new code. 2. CI Environment Setup in Dev Cluster The pipeline references the GitOps repo (specifically the Dev cluster folder). A CI folder under that Dev cluster is used to stand up a temporary environment for tests. This CI folder typically isn’t referenced by the main Dev cluster overlay, so it doesn’t affect production-like resources. The pipeline applies a Kustomization overlay that includes the new Docker image (and possibly “latest” versions of other services). 3. Readiness \u0026amp; Integration Tests Wait for Ready: The pipeline checks that all pods in the CI environment reach a “Ready” state. Integration Tests: Another folder within the CI path (e.g., integration-test) includes job manifests that run your test suite. The pipeline applies these manifests, waits for the jobs to complete, then collects logs/results. 4. Cleanup Once tests finish, the pipeline tears down the temporary namespace to keep clusters clean. If tests pass, the pipeline can merge the pull request or notify that the new image is ready for promotion. Key Benefits Modular \u0026amp; Extensible: By separating Helm charts, components, groups, and clusters, you can easily add new resources or reuse existing ones. Consistent Environments: Groups let you define and share sets of configurations across multiple stages (e.g., dev, staging, prod). Automated Testing: The CI process ensures each new feature or fix is validated in an ephemeral environment, mirroring production as closely as you need. Local Development: Tilt and k3d let you replicate the cluster environment on your machine, enabling quicker feedback loops and more productive debugging. Auditability \u0026amp; Traceability: Since every change is committed to Git, you get a clear history of who changed what and when. The Single Pane of Glass Beyond the technical benefits, this pattern fundamentally changes how teams collaborate.\nOne entry point, any role. Whether you\u0026rsquo;re a backend engineer, frontend dev, SRE, or even a PM wanting to understand the system - you clone one repo and run tilt up. Within minutes you have a working https://localhost.company.co/ with the entire stack running locally.\nCross-functional debugging. I\u0026rsquo;ve watched frontend engineers fix backend bugs by running grep -r \u0026quot;error message\u0026quot; from the repo root. The pseudo mono-repo structure means the answer is always somewhere in your checkout - you just need to find it.\nNo onboarding tax. Switching teams doesn\u0026rsquo;t mean spending a week setting up a new development environment. The same tilt up command works whether you\u0026rsquo;re on payments, auth, or the data pipeline.\nDeclarative infrastructure as documentation. New joiners can understand the entire system topology by reading the clusters/ and groups/ directories. The infrastructure is the documentation.\nConclusion Adopting GitOps with a well-thought-out repository structure can dramatically simplify your Kubernetes workflows. By combining Helm, Helmfile, Kustomize, and tools like Flux or Argo CD, you can create modular, scalable, and testable deployments. And with a local development pipeline powered by Tilt and k3d, you can iterate quickly without sacrificing best practices.\nIf you’re looking for a GitOps pattern that balances clarity, flexibility, and collaboration, give this structure a try. You’ll enjoy:\nFewer manual steps. A predictable CI process. An environment that’s friendly for both new and experienced team members. Ready to dive deeper? Experiment with a small cluster or side project first. Once you’re comfortable with the structure, scale it up to your full production workloads. Happy deploying!\nFurther Reading\nFlux CD Helm Kustomize Tilt k3d Docs ","date":"February 25, 2025","hero":"/posts/gitops/hero.jpg","permalink":"/posts/gitops/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003cbr\u003e\nIf you’ve spent any time working with Kubernetes, you’ve probably heard of \u003cem\u003eGitOps\u003c/em\u003e -a methodology that treats Git as the source of truth for defining and operating infrastructure and applications. In this post, I’ll walk you through a GitOps setup that uses a hierarchical folder structure, combining Helm, Helmfile, and Kustomize to give you robust, testable, and scalable deployments. We’ll also see how tools like Flux and Tilt fit into the workflow, enabling both automated deployments and seamless local development.\u003c/p\u003e","tags":null,"title":"Practical GitOps Pattern"},{"categories":null,"contents":"Introduction A common pattern I see promoted is using tools that show you what will change in your cluster at sync time - after your code is already merged. In my view, this is already too late and goes against GitOps principles. How can Git be the source of truth if there are extra steps between merge and understanding impact?\nIn this post, I\u0026rsquo;ll show you how to generate manifest diffs during PR review, so reviewers see exactly what will change in the cluster before they approve.\nThe Problem Consider the typical Argo CD diff plugin workflow:\nDeveloper creates PR Reviewer approves (without seeing cluster impact) Code merges Now you see the diff of what will change Sync happens The diff comes too late. The reviewer has already approved. If something looks wrong, you need another PR to fix it.\nGitOps should mean: what\u0026rsquo;s in Git is what\u0026rsquo;s in the cluster. Reviewers should understand the full impact before approving.\nThe Solution Generate diffs at PR time by:\nBuilding manifests from the main branch Building manifests from the PR branch Diffing the two Posting the result as an artifact or comment This way, reviewers see the actual Kubernetes resources that will change - deployments, services, configmaps, everything.\nKey Principle: Absorb Your Helm Charts Before we can diff manifests, we need actual manifests to diff. If you\u0026rsquo;re deploying Helm charts directly via helm install, you don\u0026rsquo;t have rendered manifests in Git.\nInstead, absorb them:\nhelm template my-release my-chart \\ --namespace my-namespace \\ \u0026gt; components/my-service/manifests.yaml Or better, use Helmfile to declaratively manage this:\n# helmfile.yaml - keep charts vanilla, use inline values only releases: - name: prometheus namespace: monitoring chart: prometheus-community/prometheus version: 25.0.0 # Minimal inline values - external values files don\u0026#39;t work with template Then render to disk:\nhelmfile template \u0026gt; components/monitoring/prometheus.yaml Important: Keep absorbed charts as vanilla as possible. Don\u0026rsquo;t try to configure everything via Helm values. Instead, push customisation to Kustomize overlays where changes are visible and diffable:\n# groups/monitoring/kustomization.yaml resources: - ../../components/monitoring/prometheus.yaml patches: - patch: |- - op: replace path: /spec/replicas value: 3 target: kind: Deployment name: prometheus-server This separation means:\nComponents: Vanilla absorbed charts (rarely change) Groups/Clusters: Environment-specific patches (visible in diffs) Building and Diffing The core CI logic is straightforward:\n# Build manifests from main branch git checkout origin/main kustomize build clusters/prod \u0026gt; /tmp/main-manifests.yaml # Build manifests from PR branch git checkout $PR_BRANCH kustomize build clusters/prod \u0026gt; /tmp/pr-manifests.yaml # Generate diff diff -u /tmp/main-manifests.yaml /tmp/pr-manifests.yaml \u0026gt; diff.txt For multiple clusters or kustomization roots, iterate:\nfor root in clusters/*/; do cluster=$(basename $root) kustomize build $root \u0026gt; /tmp/main-$cluster.yaml # ... diff each done GitLab CI Example generate-diffs: rules: - if: $CI_MERGE_REQUEST_IID changes: - deployments/**/* script: | # Fetch main branch git fetch origin main # Build main branch manifests git checkout origin/main for root in clusters/*/; do cluster=$(basename $root) kustomize build $root \u0026gt; /tmp/main-$cluster.yaml done # Build PR branch manifests git checkout $CI_COMMIT_SHA for root in clusters/*/; do cluster=$(basename $root) kustomize build $root \u0026gt; /tmp/pr-$cluster.yaml diff -u /tmp/main-$cluster.yaml /tmp/pr-$cluster.yaml \u0026gt; diffs/$cluster.diff || true done artifacts: paths: - diffs/ when: always GitHub Actions Example name: Generate Manifest Diffs on: pull_request: paths: - \u0026#39;clusters/**\u0026#39; - \u0026#39;components/**\u0026#39; - \u0026#39;groups/**\u0026#39; jobs: diff: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 with: fetch-depth: 0 - name: Setup Kustomize uses: imranismail/setup-kustomize@v2 - name: Generate diffs run: | mkdir -p diffs for root in clusters/*/; do cluster=$(basename $root) # Build from main git checkout origin/main kustomize build $root \u0026gt; /tmp/main-$cluster.yaml 2\u0026gt;/dev/null || echo \u0026#34;\u0026#34; \u0026gt; /tmp/main-$cluster.yaml # Build from PR git checkout ${{ github.sha }} kustomize build $root \u0026gt; /tmp/pr-$cluster.yaml # Diff diff -u /tmp/main-$cluster.yaml /tmp/pr-$cluster.yaml \u0026gt; diffs/$cluster.diff || true done - name: Upload diffs uses: actions/upload-artifact@v4 with: name: manifest-diffs path: diffs/ - name: Comment on PR uses: actions/github-script@v7 with: script: | const fs = require(\u0026#39;fs\u0026#39;); const diffs = fs.readdirSync(\u0026#39;diffs\u0026#39;); let comment = \u0026#39;## Manifest Diffs\\n\\n\u0026#39;; for (const file of diffs) { const content = fs.readFileSync(`diffs/${file}`, \u0026#39;utf8\u0026#39;); if (content.trim()) { comment += `\u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;${file}\u0026lt;/summary\u0026gt;\\n\\n\\`\\`\\`diff\\n${content}\\n\\`\\`\\`\\n\u0026lt;/details\u0026gt;\\n\\n`; } } github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: comment }); Finding Kustomization Roots In simple setups (one kustomization per cluster), iterating clusters/*/ works fine. But in more complex setups with nested kustomizations, groups referencing groups, and shared components, you need to find the roots - kustomizations that aren\u0026rsquo;t referenced by any other kustomization.\nThe algorithm:\nWalk all kustomization.yaml files, mark each as a potential root For each one, look at its resources and bases Any kustomization that\u0026rsquo;s referenced by another gets marked as NOT a root What remains are true roots Here\u0026rsquo;s a bash implementation:\n#!/usr/bin/env bash # find-kustomize-roots.sh - Find kustomizations not referenced by others declare -A is_root # Find all kustomization.yaml files and mark as potential roots while IFS= read -r kfile; do is_root[\u0026#34;$kfile\u0026#34;]=1 done \u0026lt; \u0026lt;(find . -name \u0026#34;kustomization.yaml\u0026#34;) # For each kustomization, mark its references as non-roots for kfile in \u0026#34;${!is_root[@]}\u0026#34;; do dir=$(dirname \u0026#34;$kfile\u0026#34;) # Extract resources and bases from the kustomization refs=$(yq -r \u0026#39;(.resources // []) + (.bases // []) | .[]\u0026#39; \u0026#34;$kfile\u0026#34; 2\u0026gt;/dev/null) for ref in $refs; do # Resolve the referenced kustomization path ref_kustomize=$(realpath -m \u0026#34;$dir/$ref/kustomization.yaml\u0026#34; 2\u0026gt;/dev/null) ref_kustomize=\u0026#34;.${ref_kustomize#$(pwd)}\u0026#34; if [[ -f \u0026#34;$ref_kustomize\u0026#34; ]]; then is_root[\u0026#34;$ref_kustomize\u0026#34;]=0 fi done done # Output only the roots for kfile in \u0026#34;${!is_root[@]}\u0026#34;; do if [[ \u0026#34;${is_root[$kfile]}\u0026#34; == \u0026#34;1\u0026#34; ]]; then echo \u0026#34;$kfile\u0026#34; fi done Now your diff script becomes:\n# Build and diff only root kustomizations for root in $(./find-kustomize-roots.sh); do root_dir=$(dirname \u0026#34;$root\u0026#34;) name=$(echo \u0026#34;$root_dir\u0026#34; | tr \u0026#39;/\u0026#39; \u0026#39;-\u0026#39;) git checkout origin/main kustomize build \u0026#34;$root_dir\u0026#34; \u0026gt; /tmp/main-$name.yaml git checkout $PR_BRANCH kustomize build \u0026#34;$root_dir\u0026#34; \u0026gt; /tmp/pr-$name.yaml diff -u /tmp/main-$name.yaml /tmp/pr-$name.yaml \u0026gt; diffs/$name.diff || true done This ensures you diff what actually gets deployed, not intermediate layers that are only used as building blocks.\nMaking It Visual Plain diffs work, but an HTML visualization makes review easier:\nTree view of changed files/resources Side-by-side comparison Kubernetes metadata (kind, namespace, name) Collapsible sections for large changes You can build this with Go templates, React, or even a simple script that wraps diff2html.\nKey Benefits No Surprises: Reviewers see exactly what will change before approving Git Is Truth: The merged state is the deployed state - no extra steps Faster Reviews: Clear diffs mean faster, more confident approvals Catch Mistakes Early: Spot accidental changes (wrong image tag, missing resource limits) before they hit the cluster Audit Trail: Diffs become part of the PR history Conclusion Showing diffs at deployment time is too late. By the time you see the impact, the code is already merged. True GitOps means understanding the full impact during review.\nThe pattern is simple:\nAbsorb Helm charts into your repo as vanilla rendered manifests Kustomize via Kustomize overlays (not Helm values) Build kustomize roots for both branches Diff and present to reviewers Review with full knowledge of cluster impact This approach has saved me countless \u0026ldquo;oops\u0026rdquo; moments and makes PR reviews genuinely meaningful for infrastructure changes.\nFurther Reading\nPractical GitOps Pattern - Repository structure and workflow Kustomize Helmfile diff2html - For visual diff rendering ","date":"January 14, 2025","hero":"/posts/gitops-pr-diffs/hero.png","permalink":"/posts/gitops-pr-diffs/","summary":"\u003cp\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\nA common pattern I see promoted is using tools that show you what will change in your cluster \u003cem\u003eat sync time\u003c/em\u003e - after your code is already merged. In my view, this is already too late and goes against GitOps principles. How can Git be the source of truth if there are extra steps between merge and understanding impact?\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll show you how to generate manifest diffs \u003cem\u003eduring PR review\u003c/em\u003e, so reviewers see exactly what will change in the cluster before they approve.\u003c/p\u003e","tags":null,"title":"GitOps PR Diffs: Review What You Deploy"},{"categories":null,"contents":"Sometimes when you\u0026rsquo;re developing or debugging locally you need access to resources that are exposed to your cluster.\nTypically, most organisations use VPN\u0026rsquo;s to enable you to access these resources, but there\u0026rsquo;s a much easier way.\nSocat. The alpine/socat image is perfect for enabling backdoor access to private or internal services that are available to your cluster without having to set up and manage VPN\u0026rsquo;s.\nHow it works is pretty simple. We run a socat pod exposing a service that\u0026rsquo;s viewable by the pod but not by us.\nWe then run a kubectl port-forward to expose the socat forward.\nAt this point we now have access to the private service locally.\nexport PORT=5432 export ADDR=postgres export PODNAME=backdoor kubectl run --restart=Never --image=alpine/socat ${PODNAME} -- -d -d tcp-listen:${PORT},fork,reuseaddr tcp-connect:${ADDR}:${PORT} kubectl wait --for=condition=Ready pod/${PODNAME} kubectl port-forward pod/${PODNAME} ${PORT}:${PORT} You don\u0026rsquo;t need to do use socat As most of you will probably be aware using socat to expose services like this is a bit overkill, you can simply use ExternalName services instead and port-forward that.\nexport PORT=5432 export ADDR=postgres export SERVICE_NAME=backdoor cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: Service apiVersion: v1 metadata: name: ${SERVICE_NAME} spec: type: ExternalName externalName: ${ADDR} EOF kubectl port-forward service/${SERVICE_NAME} ${PORT}:${PORT} ","date":"January 22, 2021","hero":"/posts/socat/hero.jpg","permalink":"/posts/socat/","summary":"\u003cp\u003eSometimes when you\u0026rsquo;re developing or debugging locally you need access to resources that are exposed to your cluster.\u003c/p\u003e\n\u003cp\u003eTypically, most organisations use VPN\u0026rsquo;s to enable you to access these resources, but there\u0026rsquo;s a much easier way.\u003c/p\u003e\n\u003ch2 id=\"socat\"\u003eSocat.\u003c/h2\u003e\n\u003cp\u003eThe alpine/socat image is perfect for enabling backdoor access to private or internal services that are available to\nyour cluster without having to set up and manage VPN\u0026rsquo;s.\u003c/p\u003e\n\u003cp\u003eHow it works is pretty simple. We run a socat pod exposing a service that\u0026rsquo;s viewable by the pod but not by us.\u003c/p\u003e","tags":null,"title":"Using socat to backdoor via kubernetes"}]